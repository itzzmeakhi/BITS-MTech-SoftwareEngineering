{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ddf4a8bc-0e86-4a92-a4cf-7f1a3d436f92",
      "metadata": {
        "id": "ddf4a8bc-0e86-4a92-a4cf-7f1a3d436f92"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries from NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ff48caa-beae-493c-bf0e-67addd3c15d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ff48caa-beae-493c-bf0e-67addd3c15d9",
        "outputId": "049be82d-3411-4a18-c81d-53758f983f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "39557bfc-a94a-434e-8f44-c0719ad42b0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39557bfc-a94a-434e-8f44-c0719ad42b0f",
        "outputId": "dd4589ae-d8ba-4f23-8ac6-c4fc9e2809d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Ensure all necessary NLTK packages are downloaded\n",
        "#nltk.download('punkt') # Tokenization\n",
        "nltk.download('stopwords') # remoinf stopwords\n",
        "nltk.download('wordnet') # for lemmatization and semantic analysis\n",
        "nltk.download('averaged_perceptron_tagger') # for POS tagging\n",
        "nltk.download('maxent_ne_chunker') #for NER\n",
        "nltk.download('words') # for recognizing words in NER\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9cd98fee-dbde-4bed-a5b3-ba5ed99f100c",
      "metadata": {
        "id": "9cd98fee-dbde-4bed-a5b3-ba5ed99f100c"
      },
      "outputs": [],
      "source": [
        "# Sample input text\n",
        "input_text = \"Chatbots can be built using Dialogflow,RASA,LLM,SLM.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "49e6360e-4481-46d9-b3c6-5fba9192f23f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49e6360e-4481-46d9-b3c6-5fba9192f23f",
        "outputId": "149e5052-5a94-4346-bded-016b1bfc3ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Normalized Text:\n",
            "chatbots can be built using dialogflow,rasa,llm,slm.\n"
          ]
        }
      ],
      "source": [
        "# 1. Normalization: Converting text to lowercase\n",
        "normalized_text = input_text.lower()\n",
        "print(\"\\n1. Normalized Text:\")\n",
        "print(normalized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7c53d957-06a0-4e7a-8b40-910399be85f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c53d957-06a0-4e7a-8b40-910399be85f1",
        "outputId": "20344915-779e-48c5-8ed1-5f67df6cdc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Tokens:\n",
            "['chatbots', 'can', 'be', 'built', 'using', 'dialogflow', ',', 'rasa', ',', 'llm', ',', 'slm', '.']\n"
          ]
        }
      ],
      "source": [
        "# 2. Tokenization: Splitting the text into words\n",
        "tokens = word_tokenize(normalized_text)\n",
        "print(\"\\n2. Tokens:\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fe1b7271-dcf6-4b19-a9ed-a14219cddcb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe1b7271-dcf6-4b19-a9ed-a14219cddcb2",
        "outputId": "11831cef-2949-48c3-a5f6-197471534f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Tokens After Stop Words Removal:\n",
            "['chatbots', 'built', 'using', 'dialogflow', ',', 'rasa', ',', 'llm', ',', 'slm', '.']\n"
          ]
        }
      ],
      "source": [
        "# 3. Stop Words Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"\\n3. Tokens After Stop Words Removal:\")\n",
        "print(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8c0a15-692d-4def-a627-70b4bd746fd5",
      "metadata": {
        "id": "fd8c0a15-692d-4def-a627-70b4bd746fd5"
      },
      "outputs": [],
      "source": [
        "# 4. Spell Check (Optional)\n",
        "# Here we assume the input text is correct; otherwise, spell check libraries like `pyspellchecker` can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1bf40ecf-efbb-4340-8443-65dd31fb94c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bf40ecf-efbb-4340-8443-65dd31fb94c5",
        "outputId": "21acd6aa-99b3-40c1-92f6-21cd3b7cc97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5. Lemmatized Tokens:\n",
            "['chatbots', 'built', 'using', 'dialogflow', ',', 'rasa', ',', 'llm', ',', 'slm', '.']\n"
          ]
        }
      ],
      "source": [
        "# 5. Stemming / Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "print(\"\\n5. Lemmatized Tokens:\")\n",
        "print(lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6845c1c5-7f3d-455a-b92e-6f6becced311",
      "metadata": {
        "id": "6845c1c5-7f3d-455a-b92e-6f6becced311"
      },
      "outputs": [],
      "source": [
        "# 6. Conversational Context (Optional in this simple example)\n",
        "# Context can be managed with external frameworks or memory stores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9b4eab20-9ff9-41ee-b463-387a27ac4424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b4eab20-9ff9-41ee-b463-387a27ac4424",
        "outputId": "d7c4af65-65b0-4902-9d39-cc271fded97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['chatbots', 'built', 'using', 'dialogflow', ',', 'rasa', ',', 'llm', ',', 'slm', '.']\n",
            "POS Tags: [('chatbots', 'NNS'), ('built', 'VBN'), ('using', 'VBG'), ('dialogflow', 'NN'), (',', ','), ('rasa', 'NN'), (',', ','), ('llm', 'NN'), (',', ','), ('slm', 'NN'), ('.', '.')]\n",
            "\n",
            "NER Chunks:\n",
            "(S\n",
            "  chatbots/NNS\n",
            "  built/VBN\n",
            "  using/VBG\n",
            "  dialogflow/NN\n",
            "  ,/,\n",
            "  rasa/NN\n",
            "  ,/,\n",
            "  llm/NN\n",
            "  ,/,\n",
            "  slm/NN\n",
            "  ./.)\n",
            "\n",
            "7. Named Entities:\n"
          ]
        }
      ],
      "source": [
        "# 7. Named Entity Recognition (NER)\n",
        "pos_tags = pos_tag(lemmatized_tokens)  # POS tagging\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "ner_chunks = ne_chunk(pos_tags)  # Named Entity Recognition\n",
        "print(\"\\nNER Chunks:\")\n",
        "print(ner_chunks)\n",
        "print(\"\\n7. Named Entities:\")\n",
        "for chunk in ner_chunks:\n",
        "    if isinstance(chunk, Tree):\n",
        "        print(f\"{' '.join(c[0] for c in chunk)} ({chunk.label()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c940533f-a51d-4e0e-80b4-ac452dd33f45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c940533f-a51d-4e0e-80b4-ac452dd33f45",
        "outputId": "97521d5d-72c7-4e89-b742-67b5866018a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency Distribution:\n",
            "[(',', 3), ('chatbots', 1), ('built', 1), ('using', 1), ('dialogflow', 1)]\n"
          ]
        }
      ],
      "source": [
        "# Example Output Frequency Distribution (optional visualization)\n",
        "fdist = FreqDist(lemmatized_tokens)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "print(fdist.most_common(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89d4f97a-8607-4917-9115-ef91ec23f2ec",
      "metadata": {
        "id": "89d4f97a-8607-4917-9115-ef91ec23f2ec"
      },
      "source": [
        "NER using SPACY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6e47e023-7195-4ca6-900f-db12ad47922c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e47e023-7195-4ca6-900f-db12ad47922c",
        "outputId": "f3fe02cf-1536-4350-ee00-53df93987846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entities:\n",
            "Skyscanner (ORG)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load SpaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "input_text = \"Skyscanner's chatbot helps users find and book flights, hotels, and car rentals by providing personalized travel recommendations and real-time pricing..\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(input_text)\n",
        "\n",
        "# Extract Named Entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ac847a-9c43-47b7-8149-0b5337986982",
      "metadata": {
        "id": "a3ac847a-9c43-47b7-8149-0b5337986982"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885fc4aa-c144-40f5-b887-8dfb3e30a6c4",
      "metadata": {
        "id": "885fc4aa-c144-40f5-b887-8dfb3e30a6c4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9c49eb7-6754-4891-8357-2c60f2d4d47b",
      "metadata": {
        "id": "f9c49eb7-6754-4891-8357-2c60f2d4d47b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8b8830-8994-4346-a5f5-19def928e8d6",
      "metadata": {
        "id": "aa8b8830-8994-4346-a5f5-19def928e8d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "229230b8-41ff-4c8c-9090-b2e64aae5e21",
      "metadata": {
        "id": "229230b8-41ff-4c8c-9090-b2e64aae5e21"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}